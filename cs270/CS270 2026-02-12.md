2026 02 12  
CS 270  
Quinn Snell  

---

<!-- Bro told us we ideally should have started working on the Decision Tree lab last week...except we started learning ab decision trees this week?? ðŸ˜­ðŸ˜­ðŸ˜­ -->

# Upcoming

- Lab 2 due next Saturday.
  - Decision trees.
- Next week, we'll learn about vector support machines.
- Midterm 2 is coming soon.
  - 2 parts: One in the testing center; then a coding exam (proctored by Proctorio).
  - TAs are preparing a study guide & will be hosting an exam review.
    - (Look for an announcement on Learning Suite and/or Discord.)
  - TOPICS:
    - Everything up until next Thursday's lecture.
      - Fs fs anything up until decision trees&mdash;for the concepts part (testing center) and programming exam.
      - A little bit of vector support machine stuff 


# Spiritual Thought

2 Nephi 31:19-21.

# Decision Trees (review-y and do-y)

## Review

### Entropy

From **Shannon's Information Theory**.


- Information of a msg in bits: $I(m)=-\log_2(p_m)$
  - e.g. If there are 16 equi-probable messages, $I$ for each message is $\log_2(1/16)$ = 4 bits.
- If the messages are not equiprobable, then could we represent them w/ less bits?
  - **Highest disorder (randomness) requires maximum information.**
- If there's a dataset $S$ of $c$ classes, then information for on class is: $I(c) = -\log_2(p_c)$.
  - Hence, total info of the data set is the sum of $(\text{info per class} \times \text{proportion of that class})$

```math
\text{Info}(S) = \text{Entropy}(S) = -\sum_{i=1}^{|C|} p_i \log_2(p_i)
```

### Information Gain Metric

Uhhh........

```math
\text{Info}_A(S) = \sum_{j=1}^{|C|} \frac {|S_j|}{|S|}\text{Info}(S)
```

$\text{Info}_A$ is the entropy of a "balance split" of your data. (Not rly sure what $A$ represents...but...)

```math
\text{Gain}(A) = \text{Info}(S) - \text{Info}_A(S)
```

(Gain/entropy does not handle "the statistical significance issue"...whatever that is.)

#### Entropy vs. Gini

Gini is an alternative gain metric to entropy. Its big advantage is its formulas don't use $\log$, which makes it faster for computers to calculate.

sklearn's implementation of decision trees use Gini by default.

### ID/C4 learning algorithm

1. $S$ = training set
2. Calculate gain for each remaining attribute: $\text{Gain}(A) = \text{Info}(S) - \text{Info}_A(S)$
3. bruh he moved away

### Pruning

After building a decision tree, you'll go through and "prune" a bunch of branches so that the tree's less complex&mdash;and this actually makes your model *more* accurate.

### Continuous Data

What if you're not working w/ nominal data? 

Split continuous data via binary splits. 

### Info gain favors attrs w/ many attr values

He breezed passed this slide.

## Regression w/ decision trees

<!-- sklearn: `DecisionTreeRegressor` -->

Regression w/ dec trees is not that different from classification. 

The trick is figuring out how to use dec tree to output a value instead of a class: Instead of doing an entropy calculation, you calculate error.

- Instead of looking at info (for classification), **we look at *error*** (for regression).
  - "Error" meaning $\text{SSE}$ or $\text{MSE}$.
<!-- - For reg training the score for a node is not GINI/Info impurity, but rather the $\text{SSE}$ or $\text{MSE}$ of instances represented by the node. -->
- The error for each leaf of the tree is the average error. (Not sure average of *what*, per se, but...hopefully I'll understand more when I go through the zybooks.)
  - And then you also make a *weighted* average...not sure what the weights are or where they come from or what...hopefully I'll understand more after going through the zybook for this week.

## Conclusion



- Good empirical results
- Comparable application robustness & accuracy w/ MLPs
- Fast learning (since no iterations)
- MLPs can be more natural w/ continuous inputs, while DT are natural w/ nominal inputs.
- One of the most well known of current symbolic systems.
- Something else but he breezed past it.

## DT Lab

- Nominals:
  - SK CART only accepts numeric features.
  - Fits CART fine since that is how CART thinks of nominal features anyways, breaking them into separate one-hot features for each possible feature value.
- a;sijf pashifasphfhjasd[fjpasd

<!-- Bro could he PLEASE show slides for more than FIVE SECONDS -->

### Nominal values in sklearn

- Sklearn is kinda wonky w/ nominal values. **So you have to convert nominal values to numbers.** 
  - (Which is ez to do w/ **one-hot encoding**.)
- Your lab will also having **missing values**. 
  - (Missing values are represented as `?` in dataframes.)
  - You can use a wonderful Python package called `missingno` to help you visualize what you're missing.
    - `msno.matrix(dfmissing)`
  - One way to deal with missing values is to treat "missing" as an actual value: So for a Y/N feature, you would now have three values: "Yes", "No", and "IDK".
- `pd.get_dummies(X)`.
  - Changes your dataset into a one-hot encoded dataset.
    - If you leave question marks in, then it will look at your data as strings&mdash;`"y"`, `"n"`, and `"?"`. 


`DecisionTreeRegressor(max_depth=n)`