2026 01 13  
CS 270  
Quinn Snell  

# KNN

Also called "k-Nearest Neighbor" Classification, or **KNN**.

Questions: How do you define a "neighbor", and how big is your "neighborhood".

## Instance-Based Classifiers

Given a set of classified training "records", classify an unseen test "record". (Idrk what "record" means...maybe that's smth I was supposed to have learned already from the Zybooks readings.)

- Examples
    - Rote-learner
        - Memorizes entire training data.
        - Performs classification only if attributes of record match one of the training e.g.s exactly.
    - Nearest Neighbor
        - Uses $k$ "closest" points (nearest neighbors) for performing classification.

## Nearest Neighbors Classifiers

Basic idea: If it walks like a duck and quacks like a duck, then it's *probably* a duck.

So you compute the distance btwn the test record and the training records, and you classify the test record along w/ whatever's nearest. Or smth. idrk this is hard to explain.

- NN Classifiers require three things:
    - Set of stored records.
    - **Distance Metric** to compute dist btwn records.
    - The value of $k$, **the number of nearest neighbors** to retrieve.
        - We look at number of neighbors, NOT the size (total distance) of neighbors, to determine the "size" of the neighborhood.
- To classify an unknown record:
    - **Compute distance** to other training recs.
    - Identify $k$ nearest neighbors.
    - Use class labels of nearest neighbors to determine the class label of unknown record.
        - (e.g., by taking a majority vote.)

## Classifying

- Simplest method: Pick what the majority rules.
    - Hence, we usually don't use even values for $k$.
- More advaanced: Weight based on distance.

## Computing Distance btwn Points

Two ways: Euclidean distance or Manhattan distance.

- Euclidean distance: This is just the line btwn both pts. So yippee Pythagorean theorem (or if you're fance the sqr rt of the dot prod).
- Manhattan distance: The distance without diagonals.
    - Called Manhattan distance bc it looks like you're walking city blocks in Manhattan.

If you form a right triangle connecting both points on its hypotenuse, the Euclidean distance is like measuring the length of the hypotenuse and Manhattan distance is the length of the triangle's legs.

## Voronoi Diagram

Voronoi Diagrams define the classification boundary of a point. 

In a Voronoi Diag, space is divided into cells, each with one point. For any new point added to the diagram, the cell it would be placed in indicates the already existing point it's closest to. (For KNN, that means it would take the class of the point already in that cell.)

## Choosing $k$

$k$ is the number of neighbors. i.e. it's the "size" of the neighborhood. Choosing your $k$ is a balancing act:

- Too small: Sensitive to noise.
- Too large: May include points from other classes.

So how you decide on a $k$? ...You kinda just have to play around w it. 

## Scaling Issues

- Attrs. may have to be scaled to prevent dist measures from being dominated by one of the attrs.
    - Some attrs have wider domains. 
- Solution: **NORMALIZE YOUR DATA!**

EX: Classifying a person: height of a person may var from 1.5 meters to 1.8; weight varies from 90lb to 300lb; income of a person varies from $10K to $10mil. In that scenario, the raw numerical value of the income attr would far outweigh the other attrs. So without normalizing the dist formula, the income would basically be the only thing that mattered when computing dist&mdash;the other attrs become negligible.

## k-NN is a lazy learner

- k-NN classifiers are "lazy learners"
    - It does not build models explicitly.
    - ("eager learners" are stuff like decision tree induction and rule-based systems.)

## Cost of classification

- Classifying unknown records is releatively expensive.
    - Without optimization: $O(n)$.
    - Hence there is a need for structures to retreive nearest neighbors fast.
        - This is called the "Nearest Neighbor Search problem".

### Solutions

- 2-dimensional kd-trees.
    - Size: $O(n)$
    - Depth: $O(\log n)$
    - Construction time: $O(n \log n)$
    - Query time: worst case $O(n)$, but avg case is $O(n \log n)$
    - Generalizes to $d$ dimensions. (Not sure what $d$ represents.)
- Hierarchical Navigable Small Worlds
    - Used in vector DBs.
    - Approaches $log n$.
    - Outperforms KD-tree and brute force solutions

## Attr Weighting

What happens when your attrs aren't raw numerical data??

### Value Difference Metric

Distance is measured based on *probabilities* of a data point is one attr or another.

# Logistic Regression

(This is what you did/will do in the first Zybooks lab.)

## Classification via Regression 

- Instead of predicting the class of a reOA;IJFISH OFPHASUV GASID UOSPd{VF JLZKJXCVH HIPV UDGIFI

## Logistic Function

$$f(t) = \frac {1}{1 + e^{-t}}$$

The inflection point of this curve is at $0.5$.

### Classification w/ the Logistic Function

For any point at $t$, you classify btwn two classes based on whether or not $f(t) > 0.5$.

Suppose $t = 0.25$

- $P(Class 0) = 1 - 0.25 = 0.75$
- $P(Class 1) = 0.25$

See, the cool thing is that it doesn't just tell you which of the two classes the point most likely is, it gives you the probability that the point will be either class.

## In practice

In practice, most of the time you'll just use `scikit-learn`'s logistical regression method.