2026 02 03  
CS 270  
Quinn Snell  


--- 

TS CLASS IS SO BUNS

--- 

# Bro I'm so sorry ts notes are actually so buns

## Classification vs Regression

- Classification predicts a DISCRETE CLASS for a given input. (And/or the prob of being in that class.)
- Regression predicts a CONTINUOUS VALUE based on input.

## Regression w/ kNN

kNN is by default for classification, but you can use it for regression.

- Can do reg by letting the output be the mean of the $k$ nearest neighbors.
  - Can also do WEIGHTED regression by letting the output be the weighted mean of $k$ nearest neighbors.

### Distance weighted reg

You use *weighted* 

$$f(x_q) = \frac{\sum^k_{i=1}w_if(x_i)}{\sum^k_{i=1}w_i}$$

Where&mdash;

- $f(x)$ is the output val for instance $x$.
- $w_i = {1}/{\text{dist}(x_q, x_i)^2}$
  - $w = 1$ for non-weighted.

### kNN Regressor

Uhhhmmmmm so I was trolling around on disc so idk what he talked ab for the last five slides.

### Classification vs. Regression

- Output type:
  - CLASSIFICATION: NOMINAL
  - REGRESSION: CONTINUOUS

## Regression

Regression fits the data w/ a hyperplane. (For linear regression ($n=1$), that's just a line.)

A HYPERPLANE in an $n-1$ dimensional object in $n$ dimensional space.

### Choosing regression type

Choose the type that minimizese your [SSE]((https://observablehq.com/@yizhe-ang/interactive-visualization-of-linear-regression)).

$$\sum(\text{predicted}_i - \text{actual}_i)^2 = \sum(\text{residue}_i)^2$$

Why we use SSE:

- SSE emphasizes outliers.
- With SSE, there's always one "best" fit.

> NOTE: "L1 error" and "L2 error" correspond to MSE and SSE respectively.  (I think...)

<!-- #### SSE of linear regression

$$Y = \beta_0 + \beta_1 X$$

$$\text{SSE} = \sum(y_i - \beta_0 ) -->


### Missed some stuff

So like I missed a ton of slides bc I was distracted.

## Evalauting regression

W/ classification, you either got it right or not. But w/ regression's continuous outputs, you no longer have a "you're right or you're not" error.

- MSE &mdash; "mean squared error". 
- RMSE &mdash; "ROOT mean squared error".
  - RMSE puts your errors back in the units of your data, so it's more intuitive.

$$\text{MSE} = \frac 1 n \sum^n_{i=1}(y_i - \hat y_i) ^2$$

$$\text{RMSE} = \sqrt{\frac 1 n \sum^n_{i=1}(y_i - \hat y_i)^2}$$


## Multiple linear regression

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_n X_n$$

Where&mdash;

- $Y$ is your hyperplane fitted to the data.
- $X$ are the input features.
- $\beta_n$ is the weight associated w/ $X_n$. 

Variables w/ the largest weight magnitudes have the highest correlation w/ output.

## Interpreting regression

Models tell you correlation&mdash;NOT CAUSATION.

> Do NOT ever ever ever EVER EVER EVER EVER *EVER* *EVER* *EVER* *EVER* ***EVER*** confuse correlation w/ causation!!!!!!!!!

There are other things you have to do to prove causation. A regression model alone does NOT prove causation. EVER!!!!!!!!!!!!!!!!!!!!!!

## Limits of (linear) regression

- Regression is not natural for classification.

## Logistic Regression

there was an equation but he skimmed past it so OK ig.

Logistic Regression IS good for classification. The shape it produces is a SIGMOID instead of a hyperplane.

But here's the trick: Insted of fitting a curve to the data in its original space, you **transform** the data into "log odds" space. Then you do linear regression WITHIN the log odds space. (And when you pull it out and put it back to your data's space, the shape is a sigmoid.) WOAHAHAHHHHH!!!

Log odds space:

$$\text{logit}(p) = \ln \Big( \frac p {1-p} \Big)$$

^ When you solve for $p$, you end up with the sigmoid.

The dope thing is that, bc logistic regression is based on linear regression, you have WEIGHTS for each of your input features. DOPE!

## Non-Linear Regression

We're not going to talk ab this really until next week. But it allows us to fit the data differently.
