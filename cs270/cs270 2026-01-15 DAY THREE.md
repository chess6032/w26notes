2026 01 15  
CS 270: Machine Learning  
Quinn Snell  

---


# Two (main) Approaches to Statistics

- FREQUENTIST: All about probability in the long run.
    - This is the approach most stats classes use.
- BAYESIAN: 
    - Data is fixed after it is observed.
        - Parameters & hypotheses are probability distributions. (Whatever tf that means lmao.)
    - The prob dist. that summarizes what is known before is the "prior distribution"&mdash;or just "the prior".
    - We start w/ prior beliefs, and update our beliefs based on new info.

# Bayes' Classifier

## Some background math

Snell tries to stick to $A$ and $C$ for variables: $C$ to represent the "Class" or "label", and $A$ will oft rep attrs/features of the data.

### Joint Probability

$P(A,C)$: The *intersection* of $A$ and $C$. 

Also written as $P(A \cap C)$ (I think?!??!??)

### Conditional Probability

$P(A|C)$: "The probability of $A$ given $C$."

$P(A|C) = \frac{P(A \cap C)}{P(C)}$

### Assuming $A$ is a set of Independent Attrs

IF $P(A) = P(a_1) \times P(a_2) \times P(a_3) \times \dots \times P(a_n)$

THEN $P(A|C) = P(a_1 | C) \times P(a_2 | C) \times \dots \times P(a_n | C)$

### Relationship btwn Joint & Conditional

$P(A,C) = P(A|C) \times P(C) = P(C|A) \times P(A)$

## Bayes' Theorem

After some dope algebra (look at the slides), we get this formula:

$$P(C|A) = \frac{P(A|C)P(C)}{P(A)}$$

- $P(C|A)$: Posterior prob of a class given an attr (or set of attrs).
- $P(A|C)$: Likelihood of the attr (or attrs), given that the data point is a class.
- $P(C)$: Prior prob of the class.
- $P(A)$: Prior prob (given the evidence) of the attr (or attrs).
    - Normalizes.

**Everything on the right side of that equation comes from your data.** So we use our data to get those probabilities, and then **use that to calculate the probability for a new data point** w/ certain attributes to be a certain class.

## Bayesian Classifiers

Most often we're not working w/ one attr, we're working w/ a bunch of attrs. So you want to find the value of $C$ that maximizes $P(C|A_1, A_2, \dots, A_n)$.

(So in the math, all you do is replace $A$ with $A_1, A_2, \dots, A_n$.)

$$P(C|A_1, A_2, \dots, A_n) = \frac{P(A_1, A_2, \dots, A_n | C) \times P(C)}{P(A_1, A_2, \dots, A_n)}$$

### Naieve Bayes Classifier

$$A_i \forall 1 \le i \le n \text{ are all independent} \Rightarrow P(A_1, A_2, \dots, A_n) = P(A_1) \times P(A_2) \times \dots \times P(A_n)$$ 
<!-- $$\text{and then } P(C|A_1,\dots,A_n) = \frac{P(A_1)P(A_2)\dots P(A_n)P(C)}{P(A_1)P(A_2)\dots P(A_n)}$$ -->

Uhhh and then some other stuff....and I got confused and ... UH MAYDAY MAYDAY, ABORT ABORT!!!!

## What about continuous data??

Bayes' theorem works with discrete $A$s and $C$s.

- **Discretize** the range into value bins.
    - Attr value becomes the representative of the bin.
- **Discretize** the range into binary bins.
    - One ordinal attr per bin (kind of like one-hot encoding).
    - May violate independence assumption.
- **Two-way Split** &mdash; $(A < v)$ or $(A > v)$
    - Choose only one of the two splits as new attr.
- **Probability density estimation**
    - Assume attr follows a normal dist.
    - Use data to est. params of dist.
        - (e.g., mean and stdeviation)
    - Once prob dist is known, you can use it to estimate conditional prob.