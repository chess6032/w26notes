2026 01 27  
CS 270  
Quinn Snell  

---

# Upcoming

- HW
  - Ch. 4 due Sat @ midnight.
- Labs
  - Lab 1 due Sat @ midnight.
  - Turn in the `.ipynb` notebook.
  - Next class will be an open time to work on the lab. (HUGE!)
    - Do NOT come in here having done nothing but the stuff we did last class. 
    - Use this time to work on problems you pooped up on.

# Doing your labs

- Use `scikit-learn` in individual labs.
- Program in Python in Jupyter notebooks.
  - NumPy library &mdash; great w/ arrays.
- Recommended tools & libraries
  - Colab: Google IDE for Python & Jupyter notebooks.
- Necessary libraries:
  - Pandas: Data Frames and tools are very convenient.
  - MatplotLib
  - Scikit-Learn
  - Numpy

Other notes

- Sometimes it's vague on what kind of chart to use. Snell said don't stress&mdash;but do label or axes and stuff.
- The code is really relatively straight forward. It's not the code they want you to get out of this class. It's the larning about the hyperparameters and what goes on and how you adjust things and how you play with things. Because that's the kind of knowledge you'll need out there in the industry. 

# Model performance

- How do we judge the quality of a particular model? (e.g. kNN w/ a partic hyperparam settings)

## Objective Functions: Accuracy

- Consider how accurate the model is on the data set.
  - Classification accuracy = # correct / total instances.
  - Classification error = # misclassified / total instances 
    - (= 1 - accuracy)

$$\text{Classification accuracy}=\frac{\text{\# correct}}{\text{total instances}}$$
$$\text{Classification error}=\frac{\text{\# misclassified}}{\text{total instances}}=1-\text{accuracy}$$

## Objective Functions: Error

- Two types of loss: L1 and L2. 
  - L1: Absolute loss.
  - L2: L1 squared.

$$\text{L1 loss: }\Sigma|t_j - z_j|$$
$$\text{L2 loss: }\Sigma(t_j - z_j)^2$$
$$(j \text{ indexes all outputs in the pattern})$$

- What about data that doesn't have mathematical meaning? (Nominal data(?))
  - In nominal data, pattern error is typically $1$ for a mismatch and $0$ for a match.

## Mean Squared Error (MSE)

- Mean of the Sum Squared Error (SSE).
- ^ Hence, MSE = SSE/*n*.

$$\text{MSE}=\frac{\text{SSE}}{n}$$

## Root mean squared error (RMSE)

$$\text{RMSE} = \sqrt{\text{MSE}} = \sqrt{\frac{\text{SSE}}{n}}$$

- RMSE is just the square root of MSE.
  - (Note that it's the root of the total data set, NOT the sum of the sqrt of each individual pattern MSE.)
- ^ Since we squared the error on the SSE, sqrting the MSE puts the err val back into the same units as the features and is thus more intuitive.
- ^ Hence, RMSE is the average distance (error) from the outputs *in the same scale as the features*.

## Error Values HW

SIKE! It's not an actual HW you have to do. Quinn Snell just left it in the slides for students who want to use it as practice.

## Error Surface

- Error is a function of the weights:
- ^ Graph that across different parameters, and the minimum is the rizzler spot with probs the best weights. 
- ^ The tricky part is that the error surfaces for these learning models can be quite complex and hard to interpret.

$$E = \Sigma(t_i - z_i)^2 = \Sigma(t_i - \Sigma x_jw_{ij})^2$$

# Training/Testing

Four methods commonly used for training/improvement:

1. Training set method.
2. Static split test.
3. Random split test set CV.
4. $N$-fold cross-validation.

(Those last two are the more accurate approaches.)

## Training Set Method

- Procedure:
  - Build model from the training set.
  - Compute accuracy on the same training set.
- Simple but least reliable estimate of performance on unseen data.
  - Is that training data representative of the real world?
  - (e.g. a rote learer could score 100%&mdash;but they'd fail against new data!)
- Not used as a performance metric but is often important info in understanding how a ML model learns.
- For this class: You will oft report this info in your labs and then compare it w/ how the learner does on diff data. Just remember that this is not necessarily the best practice.

## Static Training/Test set

- Static Split approach:
  - The machine learning is given two distinct datasets:
    - One is used exclusively for learning/training.
    - One is used exclusively for testing.
- ^ This gives you a way to do repeatable tests.
- Be careful of data leakage, where some of your test data leaks into your training data. This compromises the whole point of this approach&mdash;some percentage of your test accuracy will really be your training accuracy. 
  - So be careful when creating your datasets that the testing and training datasets.
- Can be used for challenges.
  - (e.g. to see how everyone does on on eparticular unseen set)
    - Used in Kaggle competitions.
- Be careful not to overfit the Test Set, because it may not be totally representative of the world, either. ("Gold Standard.")
- How much of your data you reserve for training and how much for testing varies, but typically it's 80/20. 

## Random Training/Test Set Approach

- Random split approach (aka "holdout" method):
  - Machine learner gets one single dataset.
  - The machine learner randomly splits the data into a training & test set.
  - The hopeful results:
    - Dist. of instances (w/ respect to the target class) is hopefully similar in both sets due to randomization.
    - Uhhhh missed it.
  - Uhhh in the end you have a training, validation, and testing dataset...idk what the differences btwn them are tho? Who knows.

### Code

```py
from sklearn.model_selection import train_test_split
import pandas as pd

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)

X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Now you have three sets:
# X_train, y_train (e.g. 80%): Used for training the model.
# X_val, y_val (e.g. 10%): Used for hyperparam tuning and model selection.
# X_test, y_test (e.g. 10%): Used for a final, unbiased evaluation of the chosen model.
```

## Stratisfied

Like mode 3 but makes sure that data is evenly distributed. Or smth like that idrk tbh. 


# Cross-Validation (CV)

- Cross-validation (CV) is a resampling method we use to "get a better feel for how well we're doing".
- CV uses diff portions of the data to test & train a model on diff iterations. We then avg the results of these iterations.
- W/ CV we avoid having data just used for either training or validation, and give all training data a chance to be a part of each, thus getting us better results
- ^ B/c W/o CV, maybe your validation set isn't representative of all your data.

Remember: $N$-fold CV is a better way to estimate how well our model will do on new data.

## $N$-fold CV

1. Partition the randomized dataset ($D$) into $N$ equally-sized subsets $S_1, ..., S_N$.  
2. For $k=1 \text{ to } N$:  
   1. Uhhhh missed it RIP
3. RIP

## Common vals for $N$

- The limit case where $N = |D|$ is known as *leave-one-out CV* and provides the most reliable estimate. However, it is typically only practice for small instances.
- Most common: $N = 10$. (i.e. break your training data into ten sets and run ten training runs.)
- So after doing CV, how do you decide which model to use? Typically, you just do the model that overall did better. Then you'll pick that model and train it on the whole training set, and you'll use that baby. 
